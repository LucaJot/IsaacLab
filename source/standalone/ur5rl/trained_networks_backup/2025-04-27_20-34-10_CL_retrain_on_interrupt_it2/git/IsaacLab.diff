--- git status ---
On branch main
Your branch is up to date with 'origin/main'.

Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
	modified:   source/extensions/omni.isaac.lab/omni/isaac/lab/envs/ui/base_env_window.py
	modified:   source/extensions/omni.isaac.lab_tasks/omni/isaac/lab_tasks/direct/ur5rl/agents/rsl_rl_ppo_cfg.py
	modified:   source/extensions/omni.isaac.lab_tasks/omni/isaac/lab_tasks/direct/ur5rl/plotting/plot_pos_dist_and_torque.py
	modified:   source/extensions/omni.isaac.lab_tasks/omni/isaac/lab_tasks/direct/ur5rl/ur5_rl_env.py
	modified:   source/extensions/omni.isaac.lab_tasks/omni/isaac/lab_tasks/direct/ur5rl/ur5_rl_env_cfg.py
	modified:   source/standalone/ur5rl/PACT.py
	modified:   source/standalone/ur5rl/env_utils.py

Untracked files:
  (use "git add <file>..." to include in what will be committed)
	source/extensions/omni.isaac.lab_tasks/omni/isaac/lab_tasks/direct/ur5rl/logdir/
	source/extensions/omni.isaac.lab_tasks/omni/isaac/lab_tasks/direct/ur5rl/plotting/plot_cube_pos.py
	source/extensions/omni.isaac.lab_tasks/omni/isaac/lab_tasks/direct/ur5rl/plotting/plot_cube_pos_two_in_one.py
	source/extensions/omni.isaac.lab_tasks/omni/isaac/lab_tasks/direct/ur5rl/plotting/plot_gap.py
	source/extensions/omni.isaac.lab_tasks/omni/isaac/lab_tasks/direct/ur5rl/plotting/plot_gap_torque_vel.py
	source/extensions/omni.isaac.lab_tasks/omni/isaac/lab_tasks/direct/ur5rl/plotting/pos_and_dist.png
	source/standalone/ur5rl/pretrain_CL1_iter0.json
	source/standalone/ur5rl/pretrain_CL1_iter1.json
	source/standalone/ur5rl/pretrain_CL1_iter2.json
	source/standalone/ur5rl/pretrain_CL2_iter0.json
	source/standalone/ur5rl/pretrain_CL2_iter1.json
	source/standalone/ur5rl/pretrain_CL3_iter0.json
	source/standalone/ur5rl/pretrain_CL3_iter1.json
	source/standalone/ur5rl/pretrain_CL3_iter2.json
	source/standalone/ur5rl/pretrain_CL3_iter3.json
	source/standalone/ur5rl/pretrain_CL3_iter4.json
	source/standalone/ur5rl/pretrain_CL3_iter5.json
	source/standalone/ur5rl/pretrain_CL4_iter0.json
	source/standalone/ur5rl/pretrain_CL4_iter1.json
	source/standalone/ur5rl/pretrain_CL4_iter2.json
	source/standalone/ur5rl/test_cube_robustness.py
	thesis_debug_plots.png

no changes added to commit (use "git add" and/or "git commit -a") 


--- git diff ---
diff --git a/source/extensions/omni.isaac.lab/omni/isaac/lab/envs/ui/base_env_window.py b/source/extensions/omni.isaac.lab/omni/isaac/lab/envs/ui/base_env_window.py
index 850ad0a3..eb586014 100644
--- a/source/extensions/omni.isaac.lab/omni/isaac/lab/envs/ui/base_env_window.py
+++ b/source/extensions/omni.isaac.lab/omni/isaac/lab/envs/ui/base_env_window.py
@@ -60,7 +60,11 @@ class BaseEnvWindow:
         print("Creating window for environment.")
         # create window for UI
         self.ui_window = omni.ui.Window(
-            window_name, width=400, height=500, visible=True, dock_preference=omni.ui.DockPreference.RIGHT_TOP
+            window_name,
+            width=400,
+            height=500,
+            visible=True,
+            dock_preference=omni.ui.DockPreference.RIGHT_TOP,
         )
         # dock next to properties window
         asyncio.ensure_future(self._dock_window(window_title=self.ui_window.title))
@@ -114,11 +118,20 @@ class BaseEnvWindow:
                     "label": "Rendering Mode",
                     "type": "dropdown",
                     "default_val": self.env.sim.render_mode.value,
-                    "items": [member.name for member in self.env.sim.RenderMode if member.value >= 0],
-                    "tooltip": "Select a rendering mode\n" + self.env.sim.RenderMode.__doc__,
-                    "on_clicked_fn": lambda value: self.env.sim.set_render_mode(self.env.sim.RenderMode[value]),
+                    "items": [
+                        member.name
+                        for member in self.env.sim.RenderMode
+                        if member.value >= 0
+                    ],
+                    "tooltip": "Select a rendering mode\n"
+                    + self.env.sim.RenderMode.__doc__,
+                    "on_clicked_fn": lambda value: self.env.sim.set_render_mode(
+                        self.env.sim.RenderMode[value]
+                    ),
                 }
-                self.ui_window_elements["render_dropdown"] = omni.isaac.ui.ui_utils.dropdown_builder(**render_mode_cfg)
+                self.ui_window_elements["render_dropdown"] = (
+                    omni.isaac.ui.ui_utils.dropdown_builder(**render_mode_cfg)
+                )
 
                 # create animation recording box
                 record_animate_cfg = {
@@ -127,13 +140,17 @@ class BaseEnvWindow:
                     "a_text": "START",
                     "b_text": "STOP",
                     "tooltip": "Record the animation of the scene. Only effective if fabric is disabled.",
-                    "on_clicked_fn": lambda value: self._toggle_recording_animation_fn(value),
+                    "on_clicked_fn": lambda value: self._toggle_recording_animation_fn(
+                        value
+                    ),
                 }
-                self.ui_window_elements["record_animation"] = omni.isaac.ui.ui_utils.state_btn_builder(
-                    **record_animate_cfg
+                self.ui_window_elements["record_animation"] = (
+                    omni.isaac.ui.ui_utils.state_btn_builder(**record_animate_cfg)
                 )
                 # disable the button if fabric is not enabled
-                self.ui_window_elements["record_animation"].enabled = not self.env.sim.is_fabric_enabled()
+                self.ui_window_elements["record_animation"].enabled = (
+                    not self.env.sim.is_fabric_enabled()
+                )
 
     def _build_viewer_frame(self):
         """Build the viewer-related control frame for the UI."""
@@ -149,7 +166,9 @@ class BaseEnvWindow:
         )
         with self.ui_window_elements["viewer_frame"]:
             # create stack for controls
-            self.ui_window_elements["viewer_vstack"] = omni.ui.VStack(spacing=5, height=0)
+            self.ui_window_elements["viewer_vstack"] = omni.ui.VStack(
+                spacing=5, height=0
+            )
             with self.ui_window_elements["viewer_vstack"]:
                 # create a number slider to move to environment origin
                 # NOTE: slider is 1-indexed, whereas the env index is 0-indexed
@@ -161,35 +180,48 @@ class BaseEnvWindow:
                     "max": self.env.num_envs,
                     "tooltip": "The environment index to follow. Only effective if follow mode is not 'World'.",
                 }
-                self.ui_window_elements["viewer_env_index"] = omni.isaac.ui.ui_utils.int_builder(**viewport_origin_cfg)
+                self.ui_window_elements["viewer_env_index"] = (
+                    omni.isaac.ui.ui_utils.int_builder(**viewport_origin_cfg)
+                )
                 # create a number slider to move to environment origin
-                self.ui_window_elements["viewer_env_index"].add_value_changed_fn(self._set_viewer_env_index_fn)
+                self.ui_window_elements["viewer_env_index"].add_value_changed_fn(
+                    self._set_viewer_env_index_fn
+                )
 
                 # create a tracker for the camera location
                 viewer_follow_cfg = {
                     "label": "Follow Mode",
                     "type": "dropdown",
                     "default_val": 0,
-                    "items": [name.replace("_", " ").title() for name in self._viewer_assets_options],
+                    "items": [
+                        name.replace("_", " ").title()
+                        for name in self._viewer_assets_options
+                    ],
                     "tooltip": "Select the viewport camera following mode.",
                     "on_clicked_fn": self._set_viewer_origin_type_fn,
                 }
-                self.ui_window_elements["viewer_follow"] = omni.isaac.ui.ui_utils.dropdown_builder(**viewer_follow_cfg)
+                self.ui_window_elements["viewer_follow"] = (
+                    omni.isaac.ui.ui_utils.dropdown_builder(**viewer_follow_cfg)
+                )
 
                 # add viewer default eye and lookat locations
-                self.ui_window_elements["viewer_eye"] = omni.isaac.ui.ui_utils.xyz_builder(
-                    label="Camera Eye",
-                    tooltip="Modify the XYZ location of the viewer eye.",
-                    default_val=self.env.cfg.viewer.eye,
-                    step=0.1,
-                    on_value_changed_fn=[self._set_viewer_location_fn] * 3,
+                self.ui_window_elements["viewer_eye"] = (
+                    omni.isaac.ui.ui_utils.xyz_builder(
+                        label="Camera Eye",
+                        tooltip="Modify the XYZ location of the viewer eye.",
+                        default_val=self.env.cfg.viewer.eye,
+                        step=0.1,
+                        on_value_changed_fn=[self._set_viewer_location_fn] * 3,
+                    )
                 )
-                self.ui_window_elements["viewer_lookat"] = omni.isaac.ui.ui_utils.xyz_builder(
-                    label="Camera Target",
-                    tooltip="Modify the XYZ location of the viewer target.",
-                    default_val=self.env.cfg.viewer.lookat,
-                    step=0.1,
-                    on_value_changed_fn=[self._set_viewer_location_fn] * 3,
+                self.ui_window_elements["viewer_lookat"] = (
+                    omni.isaac.ui.ui_utils.xyz_builder(
+                        label="Camera Target",
+                        tooltip="Modify the XYZ location of the viewer target.",
+                        default_val=self.env.cfg.viewer.lookat,
+                        step=0.1,
+                        on_value_changed_fn=[self._set_viewer_location_fn] * 3,
+                    )
                 )
 
     def _build_debug_vis_frame(self):
@@ -215,7 +247,9 @@ class BaseEnvWindow:
         )
         with self.ui_window_elements["debug_frame"]:
             # create stack for debug visualization
-            self.ui_window_elements["debug_vstack"] = omni.ui.VStack(spacing=5, height=0)
+            self.ui_window_elements["debug_vstack"] = omni.ui.VStack(
+                spacing=5, height=0
+            )
             with self.ui_window_elements["debug_vstack"]:
                 elements = [
                     self.env.scene.terrain,
@@ -245,7 +279,9 @@ class BaseEnvWindow:
             if not hasattr(self, "animation_log_dir"):
                 # create a new log directory
                 log_dir = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
-                self.animation_log_dir = os.path.join(os.getcwd(), "recordings", log_dir)
+                self.animation_log_dir = os.path.join(
+                    os.getcwd(), "recordings", log_dir
+                )
             # start the recording
             _ = omni.kit.commands.execute(
                 "StartRecording",
@@ -279,7 +315,9 @@ class BaseEnvWindow:
             temp_stage = Usd.Stage.Open(temp_layer)
             # update stage data
             UsdGeom.SetStageUpAxis(temp_stage, UsdGeom.GetStageUpAxis(stage))
-            UsdGeom.SetStageMetersPerUnit(temp_stage, UsdGeom.GetStageMetersPerUnit(stage))
+            UsdGeom.SetStageMetersPerUnit(
+                temp_stage, UsdGeom.GetStageMetersPerUnit(stage)
+            )
             # copy the prim
             Sdf.CreatePrimInLayer(temp_layer, source_prim_path)
             Sdf.CopySpec(source_layer, source_prim_path, temp_layer, source_prim_path)
@@ -308,8 +346,12 @@ class BaseEnvWindow:
             # print the path to the saved stage
             print("Recording completed.")
             print(f"\tSaved recorded stage to    : {stage_usd_path}")
-            print(f"\tSaved recorded animation to: {os.path.join(self.animation_log_dir, 'TimeSample_tk001.usd')}")
-            print("\nTo play the animation, check the instructions in the following link:")
+            print(
+                f"\tSaved recorded animation to: {os.path.join(self.animation_log_dir, 'TimeSample_tk001.usd')}"
+            )
+            print(
+                "\nTo play the animation, check the instructions in the following link:"
+            )
             print(
                 "\thttps://docs.omniverse.nvidia.com/extensions/latest/ext_animation_stage-recorder.html#using-the-captured-timesamples"
             )
@@ -322,7 +364,9 @@ class BaseEnvWindow:
         # Extract the viewport camera controller from environment
         vcc = self.env.viewport_camera_controller
         if vcc is None:
-            raise ValueError("Viewport camera controller is not initialized! Please check the rendering mode.")
+            raise ValueError(
+                "Viewport camera controller is not initialized! Please check the rendering mode."
+            )
 
         # Based on origin type, update the camera view
         if value == "World":
@@ -331,7 +375,9 @@ class BaseEnvWindow:
             vcc.update_view_to_env()
         else:
             # find which index the asset is
-            fancy_names = [name.replace("_", " ").title() for name in self._viewer_assets_options]
+            fancy_names = [
+                name.replace("_", " ").title() for name in self._viewer_assets_options
+            ]
             # store the desired env index
             viewer_asset_name = self._viewer_assets_options[fancy_names.index(value)]
             # update the camera view
@@ -342,10 +388,18 @@ class BaseEnvWindow:
         # access the viewport camera controller (for brevity)
         vcc = self.env.viewport_camera_controller
         if vcc is None:
-            raise ValueError("Viewport camera controller is not initialized! Please check the rendering mode.")
+            raise ValueError(
+                "Viewport camera controller is not initialized! Please check the rendering mode."
+            )
         # obtain the camera locations and set them in the viewpoint camera controller
-        eye = [self.ui_window_elements["viewer_eye"][i].get_value_as_float() for i in range(3)]
-        lookat = [self.ui_window_elements["viewer_lookat"][i].get_value_as_float() for i in range(3)]
+        eye = [
+            self.ui_window_elements["viewer_eye"][i].get_value_as_float()
+            for i in range(3)
+        ]
+        lookat = [
+            self.ui_window_elements["viewer_lookat"][i].get_value_as_float()
+            for i in range(3)
+        ]
         # update the camera view
         vcc.update_view_location(eye, lookat)
 
@@ -354,7 +408,9 @@ class BaseEnvWindow:
         # access the viewport camera controller (for brevity)
         vcc = self.env.viewport_camera_controller
         if vcc is None:
-            raise ValueError("Viewport camera controller is not initialized! Please check the rendering mode.")
+            raise ValueError(
+                "Viewport camera controller is not initialized! Please check the rendering mode."
+            )
         # store the desired env index, UI is 1-indexed
         vcc.set_view_env_index(model.as_int - 1)
 
@@ -383,7 +439,9 @@ class BaseEnvWindow:
                 model=omni.ui.SimpleBoolModel(),
                 enabled=elem.has_debug_vis_implementation,
                 checked=elem.cfg.debug_vis if elem.cfg else False,
-                on_checked_fn=lambda value, e=weakref.proxy(elem): e.set_debug_vis(value),
+                on_checked_fn=lambda value, e=weakref.proxy(elem): e.set_debug_vis(
+                    value
+                ),
             )
             omni.isaac.ui.ui_utils.add_line_rect_flourish()
 
diff --git a/source/extensions/omni.isaac.lab_tasks/omni/isaac/lab_tasks/direct/ur5rl/agents/rsl_rl_ppo_cfg.py b/source/extensions/omni.isaac.lab_tasks/omni/isaac/lab_tasks/direct/ur5rl/agents/rsl_rl_ppo_cfg.py
index 96444331..be3e3214 100644
--- a/source/extensions/omni.isaac.lab_tasks/omni/isaac/lab_tasks/direct/ur5rl/agents/rsl_rl_ppo_cfg.py
+++ b/source/extensions/omni.isaac.lab_tasks/omni/isaac/lab_tasks/direct/ur5rl/agents/rsl_rl_ppo_cfg.py
@@ -15,7 +15,7 @@ from omni.isaac.lab_tasks.utils.wrappers.rsl_rl import (
 @configclass
 class Ur5RLPPORunnerCfg(RslRlOnPolicyRunnerCfg):
     num_steps_per_env = 128
-    max_iterations = 500
+    max_iterations = 3000
     save_interval = 25
     experiment_name = "ur5_rl_direct"
     empirical_normalization = True
diff --git a/source/extensions/omni.isaac.lab_tasks/omni/isaac/lab_tasks/direct/ur5rl/plotting/plot_pos_dist_and_torque.py b/source/extensions/omni.isaac.lab_tasks/omni/isaac/lab_tasks/direct/ur5rl/plotting/plot_pos_dist_and_torque.py
index b787b395..40e6799d 100644
--- a/source/extensions/omni.isaac.lab_tasks/omni/isaac/lab_tasks/direct/ur5rl/plotting/plot_pos_dist_and_torque.py
+++ b/source/extensions/omni.isaac.lab_tasks/omni/isaac/lab_tasks/direct/ur5rl/plotting/plot_pos_dist_and_torque.py
@@ -10,8 +10,16 @@ log_dir = "/home/luca/isaaclab_ws/IsaacLab/source/extensions/omni.isaac.lab_task
 episodes = sorted([f for f in os.listdir(log_dir) if f.endswith(".json")])
 all_distances = []
 all_mean_torque = []
+episode_numbers = []
 
 for episode_file in episodes:
+    # Extract episode number from filename
+    try:
+        episode_num = int(episode_file.split("_")[-1].split(".")[0])
+        episode_numbers.append(episode_num)
+    except ValueError:
+        continue  # Skip if filename format is incorrect
+
     with open(os.path.join(log_dir, episode_file), "r") as f:
         data = json.load(f)
         all_distances.append(data["dist_cube_cam"])
@@ -33,19 +41,46 @@ axs[0].set_title("Cube Distance Over Time", fontsize=11)
 axs[0].grid(True, linestyle="--", linewidth=0.5)
 axs[0].set_ylim(0, None)
 
-# ✅ Plot 2: Mean Torque Over Time (Clipped to ±200 N)
+# ✅ Plot 2: Mean Torque Over Time (Clipped to ±400 N)
+plotted_penalty = False
+plotted_no_penalty = False
+
 for i in range(len(all_mean_torque)):
     steps = np.arange(len(all_mean_torque[i]))
+
+    # Determine linestyle and color based on episode number
+    if episode_numbers[i] >= 800:
+        linestyle, color, label = "-", "blue", "Torque Penalty"
+        if plotted_penalty:
+            label = None  # Avoid duplicate legend entry
+        else:
+            plotted_penalty = True
+    else:
+        linestyle, color, label = "--", "orange", "No Penalty"
+        if plotted_no_penalty:
+            label = None  # Avoid duplicate legend entry
+        else:
+            plotted_no_penalty = True
+
     axs[1].plot(
-        steps, np.clip(all_mean_torque[i], -400, 400), color="purple", linewidth=1
+        steps,
+        np.clip(all_mean_torque[i], -400, 400),
+        color=color,
+        linewidth=1,
+        linestyle=linestyle,
+        label=label,  # ✅ Assign label for legend
     )
 
+# ✅ Configure the second plot (Torque)
 axs[1].set_xlabel("Episode Steps")
-axs[1].set_ylabel("Mean Torque (N)")
+axs[1].set_ylabel("Mean Torque (Nm)")
 axs[1].set_title("Mean Torque Over Time", fontsize=11)
 axs[1].grid(True, linestyle="--", linewidth=0.5)
 axs[1].set_ylim(0, 200)  # ✅ Clipping the torque values
 
+# ✅ Add the legend
+axs[1].legend(loc="upper right", fontsize=10)
+
 # ✅ Reduce whitespace to optimize space in the thesis figure
 plt.tight_layout(pad=1.5)
 
@@ -57,4 +92,4 @@ plt.savefig(save_path_distance_torque, dpi=300)
 plt.close(fig)
 
 # ✅ Return the path where the figure is saved
-save_path_distance_torque
+print(f"Plot saved at: {save_path_distance_torque}")
diff --git a/source/extensions/omni.isaac.lab_tasks/omni/isaac/lab_tasks/direct/ur5rl/ur5_rl_env.py b/source/extensions/omni.isaac.lab_tasks/omni/isaac/lab_tasks/direct/ur5rl/ur5_rl_env.py
index 71faeb74..f9851149 100644
--- a/source/extensions/omni.isaac.lab_tasks/omni/isaac/lab_tasks/direct/ur5rl/ur5_rl_env.py
+++ b/source/extensions/omni.isaac.lab_tasks/omni/isaac/lab_tasks/direct/ur5rl/ur5_rl_env.py
@@ -269,9 +269,11 @@ class HawUr5Env(DirectRLEnv):
             "num_buckets": 250,
         }
 
-    def set_train_mode(self):
-        self.randomize = True
+    def set_randomization(self, randomize: bool):
+        self.randomize = randomize
+        return True
 
+    def set_train_mode(self):
         self.cfg.events.robot_joint_stiffness_and_damping.params = {
             "asset_cfg": SceneEntityCfg(
                 name="ur5",
@@ -332,6 +334,7 @@ class HawUr5Env(DirectRLEnv):
             self.robot.data.default_joint_pos = joint_init_state.repeat(
                 self.scene.num_envs, 1
             )
+            self.set_joint_angles_absolute(joint_angles=joint_angles)
             return True
 
     def get_joint_pos(self):
@@ -573,7 +576,7 @@ class HawUr5Env(DirectRLEnv):
             & right_contact
             & cube_contact
             & ~container_contact
-            & (self.dist_cube_cam > 0.16)
+            & (self.dist_cube_cam > 0.14)
             & (self.data_age < 3.0)
         )
 
@@ -582,14 +585,14 @@ class HawUr5Env(DirectRLEnv):
             & cube_contact
             & ~right_contact
             & ~container_contact
-            & (self.dist_cube_cam > 0.16)
+            & (self.dist_cube_cam > 0.14)
             & (self.data_age < 3.0)
         ) | (
             right_contact
             & cube_contact
             & ~left_contact
             & ~container_contact
-            & (self.dist_cube_cam > 0.16)
+            & (self.dist_cube_cam > 0.14)
             & (self.data_age < 3.0)
         )
 
@@ -599,7 +602,7 @@ class HawUr5Env(DirectRLEnv):
 
         if torch.any(partial_grasp_success):
             grasp_idx = torch.where(partial_grasp_success)[0]
-            print(f"Partial grasp in envs: {grasp_idx}")
+            # print(f"Partial grasp in envs: {grasp_idx}")
 
         grasp_success = grasp_success.squeeze(-1)
         partial_grasp_success = partial_grasp_success.squeeze(-1)
@@ -937,9 +940,7 @@ class HawUr5Env(DirectRLEnv):
         try:
             # Set arm joint angles from list
             T_arm_angles = torch.tensor(joint_angles[:6], device=self.device)
-            T_arm_angles = T_arm_angles.unsqueeze(1)
-            # Set gripper joint angles from list
-            T_arm_angles = torch.transpose(T_arm_angles, 0, 1)
+            T_arm_angles = T_arm_angles.repeat(self.scene.num_envs, 1)
 
             default_velocities = self.robot.data.default_joint_vel
 
@@ -948,6 +949,7 @@ class HawUr5Env(DirectRLEnv):
             print(f"Setting joint angles to: {T_arm_angles}")
             print(f"Shape of joint angles: {T_arm_angles.shape}")
             self.robot.write_joint_state_to_sim(T_arm_angles, default_velocities[:, :6], self._arm_dof_idx, None)  # type: ignore
+            self.jointpos_script_GT = T_arm_angles.clone()
             return True
         except Exception as e:
             print(f"Error setting joint angles: {e}")
@@ -1095,48 +1097,48 @@ def compute_rewards(
         torque_limit_exceeded_penalty_scaling * torque_limit_exceeded
     )
     pickup_reward = torch.where(
-        (grasp_success == True) & (dist_cube_cam > 0.16),
+        (grasp_success == True) & (dist_cube_cam > 0.14),
         torch.tensor(1.0, dtype=cube_z.dtype, device=cube_z.device),
         torch.tensor(0.0, dtype=cube_z.dtype, device=cube_z.device),
     )
 
-    # partial_grasp_reward = torch.where(
-    #     (partial_grasp == True) & (dist_cube_cam > 0.16) & (grasp_success == False),
-    #     torch.tensor(1.0, dtype=cube_z.dtype, device=cube_z.device),
-    #     torch.tensor(0.0, dtype=cube_z.dtype, device=cube_z.device),
-    # )
+    partial_grasp_reward = torch.where(
+        (partial_grasp == True) & (dist_cube_cam > 0.16) & (grasp_success == False),
+        torch.tensor(1.0, dtype=cube_z.dtype, device=cube_z.device),
+        torch.tensor(0.0, dtype=cube_z.dtype, device=cube_z.device),
+    )
 
-    # partial_grasp_reward = partial_grasp_reward * partial_grasp_reward_scaling
+    partial_grasp_reward = partial_grasp_reward * partial_grasp_reward_scaling
 
     pickup_reward = pickup_reward * pickup_reward_scaling
 
-    # open_gripper_incentive = torch.where(
-    #     (dist_cube_cam > 0.22) & (dist_cube_cam < 0.4) & (gripper_action_bin > 0),
-    #     torch.tensor(-0.005, dtype=dist_cube_cam.dtype, device=dist_cube_cam.device),
-    #     torch.tensor(0.0, dtype=dist_cube_cam.dtype, device=dist_cube_cam.device),
-    # )
+    open_gripper_incentive = torch.where(
+        (dist_cube_cam > 0.22) & (dist_cube_cam < 0.4) & (gripper_action_bin > 0),
+        torch.tensor(-0.005, dtype=dist_cube_cam.dtype, device=dist_cube_cam.device),
+        torch.tensor(0.0, dtype=dist_cube_cam.dtype, device=dist_cube_cam.device),
+    )
 
-    # close_gripper_incentive = torch.where(
-    #     (dist_cube_cam > 0.18) & (dist_cube_cam < 0.22) & (gripper_action_bin > 0),
-    #     torch.tensor(0.01, dtype=dist_cube_cam.dtype, device=dist_cube_cam.device),
-    #     torch.tensor(0.0, dtype=dist_cube_cam.dtype, device=dist_cube_cam.device),
-    # )
+    close_gripper_incentive = torch.where(
+        (dist_cube_cam > 0.18) & (dist_cube_cam < 0.22) & (gripper_action_bin > 0),
+        torch.tensor(0.01, dtype=dist_cube_cam.dtype, device=dist_cube_cam.device),
+        torch.tensor(0.0, dtype=dist_cube_cam.dtype, device=dist_cube_cam.device),
+    )
 
-    # # Container contact penalty
-    # container_contact_penalty_t = torch.where(
-    #     container_contact,
-    #     torch.tensor(1.0, device=container_contact.device),
-    #     torch.tensor(0.0, device=container_contact.device),
-    # )
-    # container_contact_penalty = (
-    #     container_contact_penalty_scaling * container_contact_penalty_t
-    # )
+    # Container contact penalty
+    container_contact_penalty_t = torch.where(
+        container_contact,
+        torch.tensor(1.0, device=container_contact.device),
+        torch.tensor(0.0, device=container_contact.device),
+    )
+    container_contact_penalty = (
+        container_contact_penalty_scaling * container_contact_penalty_t
+    )
 
-    # pickup_reward += (
-    #     open_gripper_incentive + close_gripper_incentive + partial_grasp_reward
-    # )
+    pickup_reward += (
+        open_gripper_incentive + close_gripper_incentive + partial_grasp_reward
+    )
 
-    # pickup_reward -= container_contact_penalty
+    pickup_reward -= container_contact_penalty
 
     # Exponential decay of reward with distance
     # dist_cube_cam = torch.where(
diff --git a/source/extensions/omni.isaac.lab_tasks/omni/isaac/lab_tasks/direct/ur5rl/ur5_rl_env_cfg.py b/source/extensions/omni.isaac.lab_tasks/omni/isaac/lab_tasks/direct/ur5rl/ur5_rl_env_cfg.py
index 44d4772a..4fa68851 100644
--- a/source/extensions/omni.isaac.lab_tasks/omni/isaac/lab_tasks/direct/ur5rl/ur5_rl_env_cfg.py
+++ b/source/extensions/omni.isaac.lab_tasks/omni/isaac/lab_tasks/direct/ur5rl/ur5_rl_env_cfg.py
@@ -4,7 +4,6 @@ import math
 from omni.isaac.lab.sim.spawners.spawner_cfg import RigidObjectSpawnerCfg
 import torch
 from collections.abc import Sequence
-
 import omni.isaac.lab.sim as sim_utils
 from omni.isaac.lab.assets import Articulation, ArticulationCfg
 from omni.isaac.lab.envs import DirectRLEnv, DirectRLEnvCfg
@@ -100,9 +99,9 @@ class HawUr5EnvCfg(DirectRLEnvCfg):
     goal_reached_scaling = 10.0
     approach_reward = 0.03
     pickup_reward_scaling = 5.0  # 0.03  #! was 0.2
-    partial_grasp_reward_scaling = 0.03
+    partial_grasp_reward_scaling = 0.01
     container_contact_penalty_scaling = 0.005
-    torque_limit = 1800  # was 500.0
+    torque_limit = 3000  # was 500.0
 
     decimation = 2
     action_scale = 0.5
diff --git a/source/standalone/ur5rl/PACT.py b/source/standalone/ur5rl/PACT.py
index 3fece86c..72324c5b 100644
--- a/source/standalone/ur5rl/PACT.py
+++ b/source/standalone/ur5rl/PACT.py
@@ -432,8 +432,8 @@ def train_CL_agent(
 
     max_cl = max(curriculum_thresholds.keys())
 
-    env.unwrapped.set_train_mode()
     current_cl = start_cl
+    env.unwrapped.set_train_mode()
 
     while current_cl <= max_cl:
         recent_rewards = []  # reset reward history per CL
@@ -503,16 +503,16 @@ def main():
     """Main function."""
 
     # Get init state from real hw or stored state
-    use_real_hw = True
+    use_real_hw = False
     # Pretrain the model
-    pretrain = False
+    pretrain = True
     # Resume the last training
     resume = True
     start_cl = 3
     EXPERIMENT_NAME = "_"
-    NUM_ENVS = 2  #  28
-    ACTION_SCALE_REAL = 0.04
-    RANDOMIZE = False
+    NUM_ENVS = 30  #  28
+    ACTION_SCALE_REAL = 0.03
+    RANDOMIZE = True
 
     # Set the goal state of the cube
     cube_goal_pos = [1.0, -0.1, 0.8]
@@ -555,15 +555,25 @@ def main():
             -0.0030048529254358414,
             -1.0,
         ]
-        cube_pos = [0.93, 0.0, 0.57]
+        # real_joint_angles = [
+        #     -0.1609,  # Gelenk 1
+        #     -0.8067,  # Gelenk 2
+        #     0.6360,  # Gelenk 3
+        #     -1.9869,  # Gelenk 4
+        #     -1.5406,  # Gelenk 5
+        #     -0.0031,  # Gelenk 6
+        # ]
+        cube_pos = [0.9, 0.0, 0.57]
 
     # env_cfg.cube_init_state = cube_pos  # type: ignore
     env_cfg.arm_joints_init_state = real_joint_angles[:-1]  # type: ignore
     env_cfg.cube_init_state = cube_pos  # type: ignore
     if use_real_hw:
-        env_cfg.action_scale = 1.0  # type: ignore to increase overall execution speed
-        env_cfg.CL_state = 1  # type: ignore
-        env_cfg.episode_length_s = 15  # was 90000
+        env_cfg.action_scale = 0.5  # type: ignore to increase overall execution speed
+        env_cfg.CL_state = 3  # type: ignore
+        env_cfg.episode_length_s = 12  # was 90000
+
+    env_cfg.episode_length_s = 9  # was 9
 
     # Create digital twin with the real-world state
     env = gym.make(
@@ -603,10 +613,18 @@ def main():
     # ------------------- CORE WORKFLOW INIT -----------------------------
 
     while True:
+        success_sim = False
+        interrupt_sim = False
+        time_out_sim = False
+
+        success_real = False
+        interrupt_real = False
+        time_out_real = False
+
         env.unwrapped.set_eval_mode()  # type: ignore
 
         # VALIDATION RUN:
-        success, interrupt, time_out, obs, policy = run_task_in_sim(
+        success_sim, interrupt_sim, time_out_sim, obs, policy = run_task_in_sim(
             env,
             log_dir=log_dir,
             resume_path=resume_path,
@@ -616,38 +634,97 @@ def main():
         obs = obs.squeeze()
         obs = obs[0]
 
-        print(f"Success: {success}")
-        print(f"Interrupt: {interrupt}")
-        print(f"Time out: {time_out}")
+        print(f"Success: {success_sim}")
+        print(f"Interrupt: {interrupt_sim}")
+        print(f"Time out: {time_out_sim}")
         # print(f"Obs: {obs}")
         # interrupt = True  #! Force Retrain for Debug
+        # success_sim = True  #! Force Success for Debug
 
         # REAL-WORLD RUN:
-        if success and use_real_hw:
+        debug_step_counter = 0
+        if success_sim and use_real_hw:
+            debug_step_counter += 1
             print_boxed("🎯 Task solved in Simulation!", color="green")
             print(colored("🤖 Moving network control to real robot...", "cyan"))
-            success = False
-            interrupt = False
-            time_out = False
 
-            while not (success or interrupt or time_out):
-                success, interrupt, time_out, obs = rg_node.step_real(
+            while not (success_real or interrupt_real or time_out_real):
+                success_real, interrupt_real, time_out_real, obs = rg_node.step_real(
                     policy, action_scale=ACTION_SCALE_REAL
                 )
             obs = obs.squeeze()
-        if success and not use_real_hw:
+            print(f"Steps: {debug_step_counter}")
+        elif success_sim and not use_real_hw:
             print_boxed("🎯 Task solved in Simulation!", color="green")
             print(colored("🤖 Real Hardware is disabled -> Done", color="cyan"))
             break
+        elif time_out_sim:
+            print_boxed("⏰ Time out reached in Simulation!", color="red")
+            print_boxed("🔄 Retraining the agent...", color="blue", symbol="=")
+            # Reset to initial training state
+            joint_angles = [
+                -0.15472919145692998,
+                -1.8963201681720179,
+                1.5,
+                -2.460175625477926,
+                -1.5792139212237757,
+                -0.0030048529254358414,
+            ]
+            gripper = [-1.0] * NUM_ENVS
+            env.unwrapped.set_gripper_action_bin(gripper)
+            env.unwrapped.set_arm_init_pose(joint_angles)
+            env.unwrapped.set_eval_mode()
+            curriculum_thresholds = {4: 5.0}
+            start_cl = 3
+            train_CL_agent(
+                env=env,
+                env_cfg=env_cfg,
+                agent_cfg=agent_cfg,
+                start_cl=start_cl,
+                resume=resume,
+                curriculum_thresholds=curriculum_thresholds,
+            )
+        elif interrupt_sim:
+            print_boxed(
+                "🚨 Interrupt received initiating interrupt training around last known state!",
+                color="red",
+            )
+            env.unwrapped.set_randomization(False)
+            joint_angles = obs[0:6].cpu().numpy()
+            gripper = [-1.0] * NUM_ENVS
+            env.unwrapped.set_gripper_action_bin(gripper)
+            env.unwrapped.set_arm_init_pose(joint_angles)
+            curriculum_thresholds = {4: 5.0}
+            start_cl = 3
+            train_CL_agent(
+                env=env,
+                env_cfg=env_cfg,
+                agent_cfg=agent_cfg,
+                start_cl=start_cl,
+                resume=resume,
+                curriculum_thresholds=curriculum_thresholds,
+            )
+        else:
+            print_boxed("❗ Termination without event! Exiting loop...", color="red")
+            break
 
-        # STATE SPECIFIC RETRAINING:
-        if interrupt:
-            print_boxed("🚨 Interrupt received, stopping the robot!", color="red")
+        if success_real:
+            print_boxed("🎯 Task solved on Real Robot!", color="green")
+            # Move to the next task
+            break
+
+        # STATE SPECIFIC RETRAINING AFTER REAL-WORLD RUN:
+        if interrupt_real:
+            print_boxed(
+                "🚨 Interrupt received on real hardware - initiating interrupt training around last known state!",
+                color="red",
+            )
             joint_angles = obs[0:6].cpu().numpy()
             gripper = bool(obs[7].cpu().numpy())
         # GENERAL RETRAINING:
-        elif time_out:
+        elif time_out_real:
             print_boxed("⏰ Time out reached!", color="yellow")
+            print(colored("Initiating general retraining...", "yellow"))
             joint_angles = [
                 -0.15472919145692998,
                 -1.8963201681720179,
@@ -667,7 +744,7 @@ def main():
 
         print_boxed("🔄 Retraining the agent...", color="blue", symbol="=")
         curriculum_thresholds = {4: 5.0}
-        start_cl = 4
+        start_cl = 3
         train_CL_agent(
             env=env,
             env_cfg=env_cfg,
diff --git a/source/standalone/ur5rl/env_utils.py b/source/standalone/ur5rl/env_utils.py
index bbf6b9dc..12e31689 100644
--- a/source/standalone/ur5rl/env_utils.py
+++ b/source/standalone/ur5rl/env_utils.py
@@ -194,18 +194,18 @@ def run_task_in_sim(
                 0
             ].item()
 
-            if grasp_success:
-                print("Grasp Successful!")
-                return True, False, False, obs, policy
+            # if grasp_success:
+            #     print("Grasp Successful!")
+            #     return True, False, False, obs, policy
             if time_out:  # type: ignore
                 print("Time Out!")
                 return False, False, True, obs, policy
-            if torque_limit_exceeded:
-                print("Torque Limit Exceeded!")
-                return False, True, False, obs, policy
-            elif dones[0]:  # type: ignore
-                print("Unexpected termination!")
-                return False, False, False, obs, policy
+            # if torque_limit_exceeded:
+            #     print("Torque Limit Exceeded!")
+            #     return False, True, False, obs, policy
+            # elif dones[0]:  # type: ignore
+            #     print("Unexpected termination!")
+            #     return False, False, False, obs, policy
 
             # last_obs = obs.clone()
             # torch.save(last_obs, os.path.join(log_dir, "last_obs.pt"))